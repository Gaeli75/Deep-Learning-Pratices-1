{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMpL9YoW0r3spBMIOpyRRtp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gaeli75/Deep-Learning-Pratices-1/blob/main/DP_learning_week_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItPyNTck6wpS",
        "outputId": "79148e59-7724-4c39-e8b5-22b9d182be16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightning-utilities in /usr/local/lib/python3.11/dist-packages (0.12.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.11/dist-packages (from lightning-utilities) (24.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities) (75.1.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install lightning-utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics --no-deps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5ZU9UQ_8dyE",
        "outputId": "586a63f4-b372-4cc4-c093-d6d41f0da9cf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchmetrics import MeanMetric, Accuracy\n",
        "from torchmetrics import ConfusionMatrix, Accuracy, Precision, Recall, F1Score"
      ],
      "metadata": {
        "id": "EPQbVWB-7Qcl"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available()\\\n",
        "     else \"mps\" if torch.mps.is_available()\\\n",
        "     else \"cpu\"\n",
        "print(\"device\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcu7dKsO7dAE",
        "outputId": "3cee85ce-5d11-483b-b241-c42d4ed1272d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing Data for Training Data\n",
        "train_data = torchvision.datasets.CIFAR10( root = './data', train = True,\n",
        "    transform = ToTensor(),\n",
        "    download = True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZMkKpYv-8w_",
        "outputId": "b3c6fd05-b2df-40ce-e182-d19b1897f570"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROTF0z0V_CjJ",
        "outputId": "7d37fd06-b439-459c-e065-31cff465df35"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 50000\n",
              "    Root location: ./data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = torchvision.datasets.CIFAR10( root = './data', train = False,\n",
        "    transform = ToTensor(),\n",
        "    download = True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCLNy5_e_JUP",
        "outputId": "65b18c31-6cb5-4d90-ac47-97c825097c8c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBN2kkQ6_yuV",
        "outputId": "042de78b-6d2b-4118-fbbe-3832fc817c78"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 10000\n",
              "    Root location: ./data\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, val_set = torch.utils.data.random_split(train_data, [0.8, 0.2])"
      ],
      "metadata": {
        "id": "0Z9tpJ8Y_1_q"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_set, batch_size=32, shuffle=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "_tjE9nX-AUmd"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.LazyConv2d(64, kernel_size=3, stride = 1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size = 2),\n",
        "    nn.LazyConv2d(128, kernel_size=3, stride =1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2),\n",
        "    nn.Flatten(),\n",
        "    nn.LazyLinear(512),\n",
        "    nn.ReLU(), # Changed nn.ReLu() to nn.ReLU()\n",
        "    nn.LazyLinear(256),\n",
        "    nn.ReLU(), # Changed nn.ReLu() to nn.ReLU()\n",
        "    nn.LazyLinear(10),\n",
        ")"
      ],
      "metadata": {
        "id": "XykfkFiJRi46"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "k521sRTtWbYM"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterian = torch.nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "oBmDWsrMXlpC"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005)"
      ],
      "metadata": {
        "id": "lX7Dj_pCXuMR"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Model\n",
        "def train_one_epoch():\n",
        "    #preparing the data\n",
        "    losses = MeanMetric().to(device)\n",
        "    acc  = Accuracy(task = 'multiclass', num_classes = 10).to(device)\n",
        "    model.train()\n",
        "    #Loop for the model\n",
        "    for X, Y in train_dataloader:\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(X)\n",
        "        loss = criterian(pred, Y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        preds = pred.argmax(dim = 1)\n",
        "        losses.update(loss.item(), X.size(0)) # Changed loss.X.size(0) to loss.item(), X.size(0)\n",
        "        acc.update(preds, Y) # Changed pred to preds, as preds contains the predicted labels\n",
        "\n",
        "    return losses.compute().item(), acc.compute().item()\n"
      ],
      "metadata": {
        "id": "murc8G3gYd5L"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Validation Model\n",
        "def Validation_one_epoch():\n",
        "    #preparing the data\n",
        "    losses = MeanMetric().to(device)\n",
        "    acc  = Accuracy(task = 'multiclass', num_classes = 10).to(device)\n",
        "    model.eval()\n",
        "    #Loop for the model\n",
        "    for X, Y in val_dataloader:\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(X)\n",
        "        loss = criterian(pred, Y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        preds = pred.argmax(dim = 1)\n",
        "        losses.update(loss.item(), X.size(0)) # Changed loss.X.size(0) to loss.item(), X.size(0)\n",
        "        acc.update(preds, Y) # Changed pred to preds, as preds contains the predicted labels\n",
        "\n",
        "    return losses.compute().item(), acc.compute().item()"
      ],
      "metadata": {
        "id": "0mif_EOpZECn"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "for i in range(0, epochs):\n",
        "    train_loss, train_acc = train_one_epoch()\n",
        "    val_loss, val_acc = Validation_one_epoch()\n",
        "    print(\"Epoch: \", i, \"| Train Loss: \", train_loss, \"| Train Accuracy\", train_acc, \"| Val Loss: \", val_loss, \"| Val Accuracy: \", val_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34_qWl4Se-tM",
        "outputId": "d6017adc-b5f3-4cf6-c77e-077c65b8d195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0 | Train Loss:  2.2875993251800537 | Train Accuracy 0.15182499587535858 | Val Loss:  2.2194409370422363 | Val Accuracy:  0.21739999949932098\n",
            "Epoch:  1 | Train Loss:  2.0442049503326416 | Train Accuracy 0.25687500834465027 | Val Loss:  1.9510042667388916 | Val Accuracy:  0.29589998722076416\n",
            "Epoch:  2 | Train Loss:  1.867126226425171 | Train Accuracy 0.33434998989105225 | Val Loss:  1.7824804782867432 | Val Accuracy:  0.3617999851703644\n",
            "Epoch:  3 | Train Loss:  1.6931638717651367 | Train Accuracy 0.39195001125335693 | Val Loss:  1.6200928688049316 | Val Accuracy:  0.41670000553131104\n",
            "Epoch:  4 | Train Loss:  1.5571194887161255 | Train Accuracy 0.43744999170303345 | Val Loss:  1.5101985931396484 | Val Accuracy:  0.4487999975681305\n",
            "Epoch:  5 | Train Loss:  1.4658030271530151 | Train Accuracy 0.46882501244544983 | Val Loss:  1.4320054054260254 | Val Accuracy:  0.484499990940094\n",
            "Epoch:  6 | Train Loss:  1.3927955627441406 | Train Accuracy 0.4984999895095825 | Val Loss:  1.370069146156311 | Val Accuracy:  0.5069000124931335\n",
            "Epoch:  7 | Train Loss:  1.3335697650909424 | Train Accuracy 0.5204749703407288 | Val Loss:  1.3138165473937988 | Val Accuracy:  0.524399995803833\n",
            "Epoch:  8 | Train Loss:  1.2797482013702393 | Train Accuracy 0.5415250062942505 | Val Loss:  1.2633323669433594 | Val Accuracy:  0.5533999800682068\n",
            "Epoch:  9 | Train Loss:  1.2312994003295898 | Train Accuracy 0.5584499835968018 | Val Loss:  1.2174690961837769 | Val Accuracy:  0.5616999864578247\n",
            "Epoch:  10 | Train Loss:  1.1826883554458618 | Train Accuracy 0.5780500173568726 | Val Loss:  1.1689701080322266 | Val Accuracy:  0.5898000001907349\n",
            "Epoch:  11 | Train Loss:  1.137562870979309 | Train Accuracy 0.5929750204086304 | Val Loss:  1.1227174997329712 | Val Accuracy:  0.6039999723434448\n",
            "Epoch:  12 | Train Loss:  1.0905170440673828 | Train Accuracy 0.61142498254776 | Val Loss:  1.082275152206421 | Val Accuracy:  0.6187999844551086\n",
            "Epoch:  13 | Train Loss:  1.049491047859192 | Train Accuracy 0.6262999773025513 | Val Loss:  1.0403741598129272 | Val Accuracy:  0.633899986743927\n",
            "Epoch:  14 | Train Loss:  1.0098429918289185 | Train Accuracy 0.6418250203132629 | Val Loss:  0.9962144494056702 | Val Accuracy:  0.6480000019073486\n",
            "Epoch:  15 | Train Loss:  0.970480740070343 | Train Accuracy 0.6581500172615051 | Val Loss:  0.9584944248199463 | Val Accuracy:  0.6628999710083008\n",
            "Epoch:  16 | Train Loss:  0.9300166964530945 | Train Accuracy 0.6736500263214111 | Val Loss:  0.9196642637252808 | Val Accuracy:  0.6740999817848206\n",
            "Epoch:  17 | Train Loss:  0.893275797367096 | Train Accuracy 0.685824990272522 | Val Loss:  0.8826599717140198 | Val Accuracy:  0.6859999895095825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JwVFjVfuh9qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Storing Evaluation metrics\n",
        "test_confusion_matrix = ConfusionMatrix(task = 'multiclass', num_classes = 10).to(device) # Moved the confusion matrix to the device\n",
        "test_accuracy = Accuracy(task = 'multiclass', num_classes = 10).to(device) # Moved the accuracy metric to the device"
      ],
      "metadata": {
        "id": "a4DAB2GmiYci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for X, Y in test_dataloader:\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "        pred = model(X)\n",
        "        preds = pred.argmax(dim = 1)\n",
        "        test_confusion_matrix.update(pred, Y)\n",
        "        test_accuracy.update(pred, Y)\n",
        "print(\"Confusion Matrix: \\n\", test_confusion_matrix.compute())\n",
        "print(\"Accuracy: \", test_accuracy.compute().item())"
      ],
      "metadata": {
        "id": "_zKxptrPj43q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q6DtDlUrjslq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}